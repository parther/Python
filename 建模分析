#! user/bin/python3
 
# 问题： 
     线性回归的 python 代码
     获得 logistics 回归分析的结果
     can't open tree.dot 无法生成决策树图表
     Invalid argument "class_mode" passed to K.function with Theano backend  model.fit(x, y, epochs = 1000, batch_size = 10)

# 重点：
     f检验 t检验，检测特征值。 https://www.zhihu.com/question/27367177
     分类和预测是预测问题的两种主要模型，分类时预测分类标号，预测是建立连续型函数模型
     分类是构造分类模型，输入样本属性值，输出对应类别，将样本映射到预先定义号的类别。属于有监督学习
     预测是建立两种或以上变量之间相互依赖的函数模型，进行预测或控制
     线性回归相对简单，当因变量和自变量存在某种曲线关系，需要建立非线性回归模型
     logistics 属于概率型非线性回归。分为二分类和多分类。二分类是，y 取0或者1。p为y取1时发生的概率
          研究的是 p 与自变量 x 的关系 
          in(p/1-p) = z =g(x)=β+β1x1+β2x2+... βnxn
          p = P(y=1 |X) = 1/(1+e^-g(x))
          回归系数，即斜率，β是回归系数
          a.iloc[1:3,:3]，前面分割行，后面分割列
          a.iloc[:,:3].as_matrix()，将pd变成 array 矩阵
          data.columns[rlr.get_support()]  index 与 数组结合 b = np.array([False, True,  True]) a[b]
          print(u'有效特征为：%s' % ','.join(['1','2','3'])) 格式化join
     决策树
          分类预测，规则提取领域广泛运用。机器学习，数据挖掘
          核心问题是每一步如何选择适当的属性对样本做拆分
          附加概率结果的一个树状的决策图
          节点表示属性的判断条件 分支表示符合节点条件的对象 树的叶子节点表示对象所属的预测结果
          ID3算法基于信息熵来选择最佳测试属性，选择当前样本中具有最大信息增益值的属性作为测试属性
          信息增益值越大，不确定越小
          给 graphviz 增加环境变量，一般是 bin 文件夹
     人工神经网络
          人工神经元是人工神经网络操作的基本信息处理单位
          人工神经网络的学习称为训练，通过训练调整参数，使其接近样本的因变量
          人工神经网络有多种不同的学习规则，需要具体问题具体分析
          误差校正学习算法是最广泛的一种
          keras的backend 设置 keras.json
          
     分类与预测算法评价
          分析与预测模型对训练集进行预测而得到的准确率并不能反映预测模型未来的性能，需要将结果运用在一组未参与预测模型建立的数据集， 评价准确率
          使用各种指标，如相对误差/绝对误差等
          python 分类预测模型的特点
          predict 方法预测结果，score（）对模型评价
          借助官方帮助文档
     聚类分析
          在没有给定分类的情况下，按照数据的相似度进行样本分组的方法
          分类模型需要使用有类别标记构成的训练数据，聚类模型可以建立在无类标记的数据上，是一种非监督的学习算法
          聚类分组的原则：按照自身距离或相似度划分为若干组，组类距离最小化而外部距离最大化
          聚类方法和聚类分析方法
          K-Means 聚类算法
               典型的基于距离的非层次聚类算法
               kind='kde' 是正态分布图，hist是直方图
          组类相似性越大，组间差别越大，聚类的效果越好
          聚类算法主要集中在scikit-learn 中
     关联规则
          又称购物篮分析，最早是为了发现超市销售数据库中不同商品之间的关联关系
          是数据挖掘中最活跃的研究方法之一，目的是找出数据集中各项之间的关联关系
          apriori 算法 先验的
          apriori 算法是最经典的挖掘频繁项集的算法，实现大数据集上可行的关联规则提取
          apriori 核心是通过连接产生候选项与其支持度，然后通过剪枝生成频繁项集
          最小支持度是统计意义上的最低重要性 
          同时满足最小值支持度阈值和最小置信度阈值的规则被称作强规则
          搜索出来的关联规则不一定有实际意义，需根据问题背景选择适当有意义的规则，并赋予合理解释
     时序模式
          按时间顺序排列的一组随机变量x1，x2，x3表示一个随机事件的时间序列 。y1，y2，y3表示该随机序列的n个有序观察值，称之为序列长度为n的观察值序列
          已知时间序列，预测序列的未来值
          时间序列的预处理
          对观察值序列的纯随机性和平稳性进行检验
          
          纯随机序列又称 白噪声序列
          平稳时间序列的定义：时间序列在某一常数附近波动且范围有限，即有常数均值和方差，延迟k期的序列变量的自协方差和自相关系数相等
          平稳序列的时序图在一个常数附近随机波动，而且波动范围有限，如果有明显趋势性或周期性就不是平稳序列了
          平稳序列具有短期相关性，延迟期数k的增加，自相关系数会在0附近随机波动
          纯随机性检验通过构造检验统计量来检验，由样本各延迟期数的自相关系数计算得出，算出对应p值，如果大于显著性水平α，则不能拒绝纯随机的原假设
          Q统计量和LB统计量用于纯随机性检验
          ARMA模型 是自回归移动平均模型，是最常用的拟合平稳序列模型
          θ 西塔 表示回归系数
          ε 艾普西龙 最小能量值ε（叫能量子）
          AR 模型 详解 https://wenku.baidu.com/view/93d8052e58fb770bf78a55a5.html
          模型定阶就是确定 p 值和 q 值
          预测时间越长 误差越大
          协方差
               方差表示一组数据分布的分散程度 ，协方差表示两组数据协同变化的程度，同向或者反向及其程度
          相关系数
               相关系数也可以看成协方差：一种剔除了两个变量量纲影响、标准化后的特殊协方差
               https://www.zhihu.com/question/20852004/answer/134902061
          自相关系数 ACF 
               同一组数据分成若干段，若干段数据的相关系数 
          偏自相关系数 拖尾 截尾
               截尾是指时间序列的自相关函数（ACF）或偏自相关函数（PACF）在某阶后均为0的性质（比如AR的PACF）
               拖尾是ACF或PACF并不在某阶后均为0的性质（比如AR的ACF）
          单位根 n次方为1的数 https://baike.baidu.com/item/%E5%8D%95%E4%BD%8D%E6%A0%B9
          单位根检验 adf 序列中存在单位根就非平稳序列了
     离群点检测
          是什么：检验数据中的异常值 
          为什么：挖掘异常数据中的信息 广泛运用于电信诈骗等安全领域
          基于统计，前提是知道数据的分布
          基于邻近度和基于密度，需要画散点图，找到离群点。基于密度是假设数据会有多个密度空间
          混合模型是基于若干个统计分析对数据建模，每个分布对应一个簇
          
          

# 教程
线性代数的本质 http://www.bilibili.com/video/av6731067/
麻省理工公开课：线性代数 http://open.163.com/special/opencourse/daishu.html
可汗学院公开课：线性代数 http://open.163.com/special/Khan/linearalgebra.html
微积分的本质 http://dwz.cn/6WjUwP 
单变量微积分 http://open.163.com/special/sp/singlevariablecalculus.html

===
向量a.b的乘积相当于向量a在向量b上投影的大小
向量a.b的叉积生成一个垂直于ab组成的平面的向量，也成为法向量，它的大小是平行四边形面积

如何通俗地解释泰勒公式？ https://www.zhihu.com/question/21149770

线性代数主要研究的是线性变换
将空间内的向量通过线性变换（矩阵）转换成其他向量
线性变换也是函数

# 01 向量是什么
向量是有序的数字列表，向量可以看成是空间上的点
在线性代数中，向量一般以原点作为起点。从原点开始，在x方向上走了多远，在y方向上走了多远
当我们用数字描述向量时，依赖于我们使用的基
选择基向量，向量的缩放和相加，可以组成其他任何向量
1 向量的加法是线性代数中唯一允许向量离开原点的情形
2 向量加法定义的理解，可以将向量放在一维，在x轴上总共走了多远
3 向量的乘法是向量乘以常数，可以看成是向量的缩放或者拉伸

# 02 线性组合 张成的空间与基
向量空间的一组基 是组成该空间的一组线性无关向量的集合。i^ j^ 是坐标系的基向量
av + bw , 向量v w的线性组合，分共线、不共线、都在原点三种情况
线性组合的集合被称为给定向量张成的空间

# 03 矩阵与线性变换
线性变换是函数的一种花哨说法，接收一个向量并输出一个向量，输入向量运动到输出向量的位置。是对基向量的变换
Ax = v，几何意义是将x变换后与v重合
线性变换本质是 基向量缩放和相加，因为基向量可以组成其他任何向量。变换后的向量是变换后基向量的线性组合，数字上用原来的坐标系表示
线性变换的性质：原点不变，保持网格线平行且等距分布（以变换后的基向量，相对原来基向量对空间的挤压伸展）
矩阵是变换后的基向量，矩阵所代表特定的线性变换

L(v) + L(w) = L(v+w)
c*L(v) = L(c*v)
 
# 04 矩阵乘法 与线性变换复合
M2M1 M2相继作用于M1的基向量[i,j] 
复合线性变化Cx 与依次线性变换ABx 的总体效应是一样的 ABC = (AB)C   
 
# 05 行列式
行列式是矩阵代表的线性变换改变面积的比例，原始面积为1 det([a b], [c d])
det([a b], [c d]) = ad-bc =0，表示矩阵代表的变换将空间压缩到更低的维度。几何意义：
1 靠近面积继续压缩缩小到负
2 对空间进行了翻转，定向改变了
三维空间行列式是线性变换改变体积的比例，三维空间用右手定律判定，大拇指k，食指i
det（A）= ad - bc，放在网格中可以形象表示，可以假设c，b为0

# 06 逆矩阵
Ax = v
逆矩阵也是线性变换，还原Ax = v中的x。通过跟踪v的动向，通过逆向变换找到x，目的是为了还原x
具体应用就是 解方程组
A^-*AX = A^-*V
A-A 等于一个什么都不做的变换，也就是基矩阵
当且仅当det(A) != 0，也就是矩阵没有将空间压缩到更低的维度，存在逆矩阵，也就是有解
因为没有矩阵能将低的维度提升到高的维度，变换只能是1对1（输入1个得到1个），而不能1对多，需要一一对应

列空间：矩阵张成的空间
秩：列空间张成的维度
零空间：Ax = 0 x的所有可能解构成的集合即为0空间

不同维度转换：
2x3 矩阵将3维度向量转换为2维向量
3x2 矩阵 的列空间是（三维空间中一个过原点的）二维空间
3x2 矩阵 的几何意义是将二维空间映射到三维空间上
     
# 07 点积与对偶性
v = [[a], [b]] w = [[c], [d]]
点积 v·w = ac+bd 坐标相乘然后相加
点积的几何意义：点积的结果与投影的关系。向量v在向量w投影的长度 乘以 向量v的长度
点积的结果与顺序无关，可以在坐标轴上用 对偶性解释
可以理解为矩阵向量相乘
          
# 08 叉积的标准介绍
     v x w = p 
     p 是v 和 w 围成的平行四边形面积大小，p 也是向量，用一个坐标表示，方向与平行四边形垂直，与v w 垂直
     从数值上看，v x w = det（[v，w]）
     方向遵循右手定律
     叉积 是 输入两个三维向量 生成一个向量
     
# 08 2 叉积
     u x v x w = det（[u , v, w]），将v w 看成常量，u = [x,y,z]为变量
     接收 u 的线性变换 [?,?,?]u = det（[u , v, w]）
     p1x + p2y + p3z = x() + y() + z()，可求出p
     找到一个特殊的向量p，使p与[x,y,z]的点积 等于 一个3x3矩阵的行列式，且第一列为[x,y,z]
     几何上看，行列式是体积大小，线性变换p将 u 投影到垂直于 v w 的平行四边形上
     
# 09 基向量的变换
     标准基向量坐标系 珍妮弗坐标系
     将 珍妮弗坐标系向量 转换为 标准基向量坐标系的向量
     
     A 为 以标准基向量坐标系的向量 表示的 珍妮弗坐标系基向量
     v 为 珍妮弗坐标系表示的向量
     Av = v0  v0为 标准基向量坐标系的向量
     A-v0 = v  v为 珍妮弗坐标系表示的向量
     
     线性变换的重要特征 线性组合不变，只是 网格变成了 以变换后基向量为单位的网格
     A-MA 模式能解决很多变换
     
# 10 特征向量与特征值
     特征向量是 线性变换矩阵 的特征向量，是此向量变换后 停留在 此向量变换前张成的空间中的向量
     所有基向量都是特征向量
     特征值用来衡量 特征向量被拉伸的程度
     求出该矩阵能使哪些向量（当然是特征向量）只发生拉伸，使其发生拉伸的程度如何（特征值大小）
     看清一个矩阵在那些方面能产生最大的效果（power），并根据所产生的每个特征向量（一般研究特征值最大的那几个）进行分类讨论与研究
     Av = λv  Av = (λI) v  Av - (λI) v = 0 (A - λI)v = 0
     当矩阵将空间压缩到更低维度的时候，才会存在非零向量，使矩阵向量乘积为零向量
     det(A - λI) = 0 求出λ
     
# 11 线性代数与微积分
     如果 f(x) + g(x) = (f + g)x，cf(x) = f(cx)，满足线性变换的基本性质，如微积分求导，那可以用线性变换来处理
     f(x) = x^3 + 5x^2 + 4x + 5
     基函数是 b0(x) = 1 b1(x) = x b2(x) = x^2 ...
     由 基函数 的导数构造 求导矩阵
     [0 1 0 0 ...] [0 0 2 0 ...] [0 0 0 3 ...] ...
线性代数基本术语 http://netclass.csu.edu.cn/NCourse/hep012/juzhen/16.htm
最小二乘法 least square estimation ：
http://open.163.com/special/Khan/linearalgebra.html
https://zhuanlan.zhihu.com/p/26032019
1 从矩阵的角度讲：
已知向量b，矩阵A。b不在A张成的空间内，Ax=b无解。试在A空间中找出向量b^，使得b^与b最接近
||b - b^||最小，求欧式距离，也就是 (b1 - b1^)^2 + (b2 - b2^)^2 + ... + (bn - bn^)^2 最小
当b^为b在A空间投影的时候，||b - b^||最小，b^与b最接近

2 从机器学习的角度讲：
最小二乘法 是 一种优化技术，找到一个函数，使预测值与数据实际值差的平方和最小
监督学习中的回归问题，求损失函数一般是平方和函数
      
向量：
向量是欧式空间内的概念，向量是空间内的点
向量是一个有序的数字列表，列表的数字表示从各个维度上方向上从原点走了多远
向量的长度：向量离原点的距离
向量的基本性质：
以二维向量为例 u = [[u1][u2]]  v = [[v1][v2]]  c为标量
u+v = [[u1+v1],[u2+v2]]
c*v = [[c*v1][c*v2]]

向量的点积：
u*v = [[u1*v1],[u2*v2]]
几何理解：u在v上的投影乘以v的长度
矩阵理解：u^T*v 矩阵u^T将向量v从2维转为1维

矩阵:
矩阵是一个有序的向量列表
矩阵代表特定的线性变换，相当于函数。输入一个向量，输出一个向量
矩阵的本质是向量的线性组合
矩阵的性质：
A*(u+v) = A*u + A*v
A*(c*v) = c*A*v
